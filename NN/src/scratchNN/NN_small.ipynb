{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas is for reading data\n",
    "import pandas as pd\n",
    "# numpy is for linear algebra\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv(\"data_files/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = data_1.iloc[0:3,0:7]\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = data.shape\n",
    "\n",
    "# In order to avoid overfitting, we want to randomize the data and then split it into train and dev\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# transpose the data so that each column is a row (easier)\n",
    "data_dev = data[0:10].T\n",
    "# each column is now an image\n",
    "# first row is now labels\n",
    "# following rows (783) are each pixel\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:]\n",
    "X_dev = X_dev / 255\n",
    "\n",
    "data_train = data.T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:]\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"orig_data: \", data.shape)\n",
    "print(data)\n",
    "print()\n",
    "print(\"data_train: \", data_train.shape)\n",
    "print(data_train)\n",
    "print('X_train: ', X_train.shape)\n",
    "print(X_train)\n",
    "print('Y_train: ', Y_train.shape)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # random.randn makes dist between -.5 and .5\n",
    "    # wi is the weight vector (# second layer neurons, # first layer neurons)\n",
    "        # each input neuron (784) connects to each output neuron (10)\n",
    "    # bi is the bias in the output layer neurons\n",
    "    w1 = np.random.randn(10,6)\n",
    "    b1 = np.random.randn(10,1)\n",
    "    w2 = np.random.randn(10,10)\n",
    "    b2 = np.random.randn(10,1)\n",
    "    print(f\"w1: {w1.shape}\")\n",
    "    print(f\"b1: {b1.shape}\")\n",
    "    print(f\"w2: {w2.shape}\")\n",
    "    print(f\"b2: {b2.shape}\")\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init_params()\n",
    "# print('w1:', w1.shape, '\\n')\n",
    "# print('b1:',b1.shape, '\\n')\n",
    "# print('w2:',w2.shape, '\\n')\n",
    "# print('b2:',b2.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    # maximum is element-wise so it runs that calc for each element in Z \n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# converts a vector of real numbers into a prob dist of K possible outcomes\n",
    "def softmax(Z):\n",
    "    print(\"sum(np.exp(Z)) : \",sum(np.exp(Z)) )\n",
    "    return np.exp(Z) / sum(np.exp(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left off at 16:54\n",
    "def forward_prop(w1, b1, w2, b2, X):\n",
    "    print(\"Forward Propogation\")\n",
    "    z1 = w1.dot(X) + b1\n",
    "    print(\"z1 = w1.dot(X) + b1\")\n",
    "    print(f\"{z1.shape} = {w1.shape} .dot {X.shape} + {b1.shape}\")\n",
    "    A1 = ReLU(z1)\n",
    "    print(\"A1 = ReLU(z1)\")\n",
    "    print(f\"{A1.shape} = Relu{z1.shape}\")\n",
    "    z2 = w2.dot(A1) + b2\n",
    "    print(\"z2 = w2.dot(A1) + b2\")\n",
    "    print(f\"{z2.shape} = {w2.shape} .dot {A1.shape} + {b1.shape}\")\n",
    "    A2 = softmax(z2)\n",
    "    print(\"A2 = softmax(z2)\")\n",
    "    print(f\"{z2.shape} = {w2.shape} .dot {A1.shape} + {b1.shape}\")\n",
    "    print(\"z1: \")\n",
    "    print()\n",
    "    print(z1)\n",
    "    print(\"A1: \")\n",
    "    print()\n",
    "    print(A1)\n",
    "    print(\"z2: \")\n",
    "    print()\n",
    "    print(z2)\n",
    "    print(\"A2: \")\n",
    "    print()\n",
    "    print(A2)\n",
    "    print()\n",
    "\n",
    "    return z1, A1, z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform a vector Y of class labels into a one-hot encoded matrix\n",
    "# one-hot encoding is a common way to represent categorical variables as binary vectors \n",
    "# Y is going to be an array (mx1) where each element is the predicted class for the equivalent instance column of the input data array\n",
    "def one_hot(Y):\n",
    "    # np.zeros line creates a 2D array of zeros with shape determined by number of samples and number of unique classes   \n",
    "        # y.size returns the total number of elements in Y which represents the number of samples or instances\n",
    "        # Y.max() + 1 calculates the max value in Y and adds 1 to determine the number of unique classes\n",
    "            # adding one is necessary because the classes start from 0 (0-9)\n",
    "    print()\n",
    "    print(\"One hot encoding\")\n",
    "    print(\"Y \", Y, Y.size)\n",
    "    ohY = np.zeros((Y.size, 10))\n",
    "    # \"for each row, go to the column specified by the label in Y and set it equal to 1\"\n",
    "    # by indexing ohY like this, we are effectively selecting one position per row, determined by the class label in Y\n",
    "    # each row in ohY corresponds to a sample in Y and each column in ohY corresponds to a class\n",
    "    # for each row in ohY, the column corresponding to its class label is set to 1 (all other columns remain 0)\n",
    "        #  np.arange(Y.size) generates an array of indices from 0 to Y.size - 1 corresponding to each sample in Y --> specifies what row to access\n",
    "        # Y contains the class label for each sample\n",
    "            # when used as an index, Y selects the column in ohY that corresponds to its class label\n",
    "    # \n",
    "    ohY[np.arange(Y.size), Y] = 1\n",
    "    print('\\n',\"ohy \", ohY)\n",
    "    print(\"ohY.T \", ohY.T, ohY.T.size)\n",
    "    # transpose because we want each column to be a sample not each row\n",
    "    return ohY.T\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    # relu has deriv of 1 for x > 0 (because x = x) and 0 for x <=0 (because x = 0)\n",
    "    # this works because booleans are converted to 1 for true and 0 for false so if a number is positive then its deriv was 1\n",
    "    # since\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(z1, A1, z2, A2, w2, X, Y):\n",
    "    print('Back Propogation')\n",
    "    m = Y.size\n",
    "    print(\"m = Y.size\", Y.shape)\n",
    "\n",
    "    ohY = one_hot(Y)\n",
    "    print(\"ohY: \", ohY.size)\n",
    "\n",
    "    dz2 = A2 - ohY\n",
    "    print(\"dz2 = A2 - ohY\")\n",
    "    print(f\"{dz2.shape} = {A2.shape} - {ohY.shape}\")\n",
    "\n",
    "    dw2 = 1/m * dz2.dot(A1.T)\n",
    "    print(\"dw2 = 1/m * dz2.dot(A1.T)\")\n",
    "    print(f\"{dw2.shape} = {1/m} * {dz2.shape} dot {A1.T.shape}\")\n",
    "    \n",
    "    db2 = 1/m * np.sum(dz2, axis=1)\n",
    "    print(\"db2 = 1/m * np.sum(dz2)\")\n",
    "    print(f\"{db2.shape} = {1/m} * np.sum{dz2.shape}\")\n",
    "    \n",
    "    # I don't understand this next part\n",
    "    dz1 = w2.T.dot(dz2) * deriv_ReLU(z1)\n",
    "    print(\"dz1 = w2.T.dot(dz2) * deriv_ReLU(z1)\")\n",
    "    print(f\"{dz1.shape} = {w2.T.shape} dot {dz2.shape} * deriv_ReLU{z1.shape}\")\n",
    "\n",
    "    dw1 = 1/m * dz1.dot(X.T)\n",
    "    print(\"dw1 = 1/m * dz1.dot(X.T)\")\n",
    "    print(f\"{dw1.shape} = {1/m} * {dz1.shape} dot {X.T.shape}\")\n",
    "\n",
    "    db1 = 1/m * np.sum(dz1, axis=1)\n",
    "    print(\"db1 = 1/m * np.sum(dz1)\")\n",
    "    print(f\"{db1.shape} = {1/m} * np.sum{dz1.shape}\")\n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, lr):\n",
    "    print(f\"w1: {w1.shape}\")\n",
    "    print(f\"b1: {b1.shape}\")\n",
    "    print(f\"w2: {w2.shape}\")\n",
    "    print(f\"b2: {b2.shape}\")\n",
    "    print(f\"dw1: {dw1.shape}\")\n",
    "    print(f\"db1: {db1.shape}\")\n",
    "    print(f\"dw2: {dw2.shape}\")\n",
    "    print(f\"db2: {db2.shape}\")\n",
    "    w1 = w1 - lr*dw1\n",
    "    b1 = b1 - lr*db1.reshape(-1,1)\n",
    "    w2 = w2 - lr*dw2\n",
    "    b2 = b2 - lr*db2.reshape(-1,1)\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleUpToA2(X, Y):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "    z1, A1, z2, A2 = forward_prop(w1, b1, w2, b2, X)\n",
    "    return z1, A1, z2, A2\n",
    "\n",
    "def singleBackprop(z1, A1, z2, A2, w2, X, Y):\n",
    "    dw1, db1, dw2, db2 = back_prop(z1, A1, z2, A2, w2, X, Y)\n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rand = pd.DataFrame([[1., 0., 0.],\n",
    "          [0., 1., 0.],\n",
    "          [0., 0., 1.],\n",
    "          [1., 0., 0.],\n",
    "          [0., 0., 1.],\n",
    "          [0., 0., 1.]])\n",
    "\n",
    "print(X_rand)\n",
    "print(X_rand.shape)\n",
    "print(Y_train)\n",
    "print(Y_train.shape)\n",
    "z1, A1, z2, A2 = singleUpToA2(X_rand, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(np.array([1, 2, 3]))[0]\n",
    "\n",
    "dw1, db1, dw2, db2 = singleBackprop(z1, A1, z2, A2, w2, X_rand, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, '\\n', Y)\n",
    "    return np.sum(predictions==Y) / Y.size\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        z1, A1, z2, A2 = forward_prop(w1, b1, w2, b2, X)\n",
    "        dw1, db1, dw2, db2 = back_prop(z1, A1, z2, A2, w2, X, Y)\n",
    "        w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)\n",
    "        if (i%10) == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(f\"Accuracy: {get_accuracy(get_predictions(A2), Y)}\")\n",
    "            print()\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = gradient_descent(X_train, Y_train, 1, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
